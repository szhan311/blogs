<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/blogs/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/blogs/" rel="alternate" type="text/html" /><updated>2025-11-11T17:58:24-08:00</updated><id>http://localhost:4000/blogs/feed.xml</id><title type="html">blogs</title><entry><title type="html">Learn Conditional Expectations via Bregman divergence</title><link href="http://localhost:4000/blogs/2025/11/11/Bregman-divergence.html" rel="alternate" type="text/html" title="Learn Conditional Expectations via Bregman divergence" /><published>2025-11-11T00:00:00-08:00</published><updated>2025-11-11T00:00:00-08:00</updated><id>http://localhost:4000/blogs/2025/11/11/Bregman-divergence</id><content type="html" xml:base="http://localhost:4000/blogs/2025/11/11/Bregman-divergence.html"><![CDATA[<h1 id="bregman-divergence">Bregman divergence</h1>

<p>The <strong>Bregman divergence</strong> measures how far two points are from each other <strong>with respect to a convex function</strong> — it generalizes familiar notions like squared Euclidean distance and Kullback–Leibler (KL) divergence.</p>

<h2 id="definition">Definition</h2>
<p>Let $\phi: \mathbb{R}^n \rightarrow \mathbb{R}$ be a strictly convex, differentiable function. The Bregman divergence between two points $x$ and $y$ is defined as:</p>

\[D_{\phi} (x, y) = \phi(x)-\phi(y) - \langle\nabla \phi(y), x-y \rangle\]

<p>where $\phi(x)$ is value of the convex function at $x$, $\nabla \phi(y)$ is gradient at $y$, $\langle \cdot, \cdot \rangle$ denotes inner product. Intuitively, $D_{\phi}(x, y)$ measures how much $\phi(x)$ exceeds the tangent plane to $\phi$ at $y$.</p>
<h2 id="examples">Examples</h2>
<p><strong>Squared Euclidean Distance</strong>. If $\phi(x)=\frac{1}{2}\Vert x \Vert^2$, then
\(D_{\phi}(x, y)=\frac12 \Vert x-y \Vert_2^2\)
So the Euclidean distance squared is a Bregman divergence.
<strong>Kullback–Leibler (KL) Divergence</strong>. If $\phi(p)=\sum_{i}p_i\log p_i$ (the negative entropy), then</p>

<p>\(D_{\phi}(p, q)=\sum_{i}p_i \log \frac{p_i}{q_i}\)
That’s the KL divergence between distributions $p$ and $q$.</p>

<h1 id="learn-conditional-expectations-via-bregman-divergence">Learn Conditional Expectations via Bregman divergence</h1>

<blockquote>
  <p>Proposition 1 [1].  Let $X\in \mathcal{S}_X, Y \in \mathcal{S}_Y$ be RVs over state spaces $\mathcal{S}_X$, $\mathcal{S}_Y$ and $g: \mathbb{R}^p \times \mathcal{S}_X \rightarrow \mathbb{R}^n$, $(\theta, x) \mapsto g^\theta(x)$, where $\theta \in \mathbb{R}^p$ denotes learnable parameters. Let $D_X(u, v),\, x \in \mathcal{S}_X$ be a Bregman divergence over a convex set $\Omega \subset \mathbb{R}^n$ that contains the image of $f$. 
Then,
\(\nabla_\theta \mathbb{E}_{X,Y}\, D_X\!\big(Y, g^\theta(X)\big)
= \nabla_\theta \mathbb{E}_X\, D_X\!\big(\mathbb{E}[Y \mid X], g^\theta(X)\big).\)
In particular, for all $x$ with $p_X(x) &gt; 0$, the global minimum of $g^\theta(x)$ with respect to $\theta$ satisfies
\(g^\theta(x) = \mathbb{E}[Y \mid X = x].\)</p>
</blockquote>

<h2 id="learn-conditional-expectation-in-diffusion-model">Learn conditional expectation in diffusion model</h2>

<p>For continuous diffusion models, $\mathbb{E}[f(x_0) \mid x_t]:= \int f(x_0) p( x_0\mid x_t) dx_0$. The conditional expectation $\mathbb{E}[f(x_0) \mid x_t]$ is the minimizer of the following objective:</p>

\[\mathbb{E}[f (x_0) \mid x_t] = \arg \min_{g(x_t, t)} \ \mathbb{E}_{x_0 \sim \pi_0} \mathbb{E}_{x_t \sim p(x_t \mid x_0)} \Vert g(x_t, t) -f(x_0)\Vert^2 \tag{1}\]

<p>Eq. (1) connect the conditional expectation with a minimizer, which is the objective for training.</p>

<p>For discrete diffusion models, we want to learn</p>

\[p_{0 \mid t}^{i}(x_0^i \mid x_t)= \mathbb{E}[\delta (X_0^i=x_0^i) \mid X_t =x_t]\]

<p>Here we use KL-divergence instead of Squared Euclidean Distance. The loss is:</p>

\[\mathbb{E}_{x_0 \sim \pi_0, x_t \sim p(x_t \mid x_0)} D \left(\delta (X_0^i=x_0^i), p_{0 \mid t}^{\theta, i}(X_0^i=x_0^i \mid X_t= x_t) \right)\]

<p>where $D(p, q)= \sum_{\alpha \in \mathcal{\tau}} p(\alpha) \log \frac{p(\alpha)}{q(\alpha)}$. Since minimize KL divergence is equivalent to maximize likelihood, we have</p>

\[\arg \min - \mathbb{E}_{x_0 \sim \pi_0, x_t \sim p(x_t \mid x_0)} \log p_{0 \mid t}^{\theta, i} (X_0^i=x_0^i \mid X_t=x_t)\]

<p>[1] Lipman Y, Havasi M, Holderrieth P, et al. Flow matching guide and code[J]. arXiv preprint arXiv:2412.06264, 2024.</p>]]></content><author><name></name></author><category term="reading notes" /><summary type="html"><![CDATA[Bregman divergence]]></summary></entry><entry><title type="html">Conditional Expectation - the core of diffusion models</title><link href="http://localhost:4000/blogs/2025/11/11/Conditional-Expectation.html" rel="alternate" type="text/html" title="Conditional Expectation - the core of diffusion models" /><published>2025-11-11T00:00:00-08:00</published><updated>2025-11-11T00:00:00-08:00</updated><id>http://localhost:4000/blogs/2025/11/11/Conditional-Expectation</id><content type="html" xml:base="http://localhost:4000/blogs/2025/11/11/Conditional-Expectation.html"><![CDATA[<p>The conditional expectation is the core to connect almost all things in diffusion models, e.g., the sampling ODE, SDE, training objective, score function and so on. This note try to understand diffusion models by conditional expecation.</p>

<h1 id="preliminary">Preliminary</h1>

<p><strong>Conditional expectation</strong>. $\mathbb{E}[f(x_0) \mid x_t]:= \int f(x_0) p( x_0\mid x_t) dx_0$.</p>

<p><strong>Diffusion model</strong>. Given training dataset $\mathcal{D}={ x_0^{i} }_{i=1}^N$ from target distribution $\pi_0(x_0)$, $x_0^{i} \in \mathbb{R}^d$, the goal of generative modelling is to draw new samples from $\pi_0$. Generally, it’s hard to directly sample from the target distribution $\pi_0$, since it is usually complex. But we can build a diffusion process to transfer a simple prior density $\pi_1$ to target density $\pi_0$ gradually, where boundary conditions are required: $p_0=\pi_0, \quad p_1 = \pi_1$. Usually we select $\pi_1$ as a Gaussian distribution: $\pi_1\sim \mathcal{N} (0, \mathbb{I})$.</p>

<h1 id="design-of-the-diffusion-process"><strong>Design of the diffusion process</strong></h1>

<p>The problem is how to design $p_t$? To answer this question, we first introduce conditional flow $p_{t \mid 0} (x_t \mid x_0)$, which is the conditional distribution of $x_t$ given $x_0$. Then we can $p_t$ as a mixture of densities: $p_t(x_t) = \int p_t(x_t \mid x_0) \pi_0(x_0) dx_0$. If we let the transition kernel $p_t(x_t \mid x_0)$ to be a Gaussian: $p_{t \mid 0} (x_t \mid x_0) = \mathcal{N} (x_t; \alpha_t x_0, \sigma_t^2 \mathbb{I})$, then the distribution $p_t$ is smoother as $t$ grows, and $p_t$ is given by:</p>

\[\begin{align}p_t(x_t) &amp;= \int p_t (x_t \mid x_0) \pi_0(x_0) dx_0\\ &amp; = \int \mathcal{N} (x_t; \alpha_tx_0, \sigma_t^2 \mathbb{I}) \pi_0(x_0) dx_0  \end{align} \tag{1}\]

<p>As we set $\alpha_0=1$, $\sigma_0\rightarrow0$, $\alpha_1 = 1$, $\sigma_1 = 1$. $p_t$ is a diffusion process that transfer from data distribution $p_0=\pi_0$ to prior (Gaussian) distribution $p_1=\pi_1=\mathcal{N} (0, \mathbb{I})$.</p>

<h1 id="from-end-conditioned-flow-to-unconditional-flow">From end-conditioned flow to unconditional flow</h1>

<p><strong>End-conditioned flow</strong>. Given $x_0\sim \pi_0$, how to sample from the transition kernel $\mathcal{N} (x_t; \alpha_t x_0, \sigma_t^2 \mathbb{I})$? Actually, we can build a flow map to do this:</p>

\[x_t = \alpha_t x_0 + \sigma_t \epsilon; \quad \epsilon \sim \mathcal{N} (0, \mathbb{I}). \tag{2}\]

<p>Alternatively, we can run an ODE from $0$ to $t$:</p>

\[dX_t = (\dot{\alpha}_t x_0 + \dot{\sigma}_t \epsilon)dt, \quad, \epsilon \sim \mathcal{N} (0, \mathbb{I}) \tag{3}\]

<p>We denote $v_{t\mid 0}=\dot{\alpha}_t x_0 + \dot{\sigma}_t \epsilon$.</p>

<p><strong>Unconditional flow</strong>. $\pi_1$ is a Gaussian distribution and easy to sample $x_1$ from $\pi_1$, so the problem is that how to transfer from $\pi_1$ to data distribution $\pi_0$? The answer is simple, we can run an ODE from $t=T$ to $t=0$:
\(dX_t = u_t (x_t)dt \tag{4}\)</p>

<p>where $v_t(x_t):=\mathbb{E}[v_{t \mid 0} \mid x_t]$.</p>

<p>To proof Eq. (4), we first introduce continuity equation.</p>

<blockquote>
  <p><strong>Theorem (Continuity Equation).</strong>  The <strong>continuity equation</strong> expresses <strong>mass (probability) conservation</strong> of a time-evolving density $p_t(x)$ under a vector field $v_t(x)$:
\(\frac{\partial p_t(x_t)}{\partial t} + \nabla_x \cdot (p_t v_t) = 0 \tag{5}\)</p>
</blockquote>

<p>By Continuity Equation (5) and Eq. (3), $p_{t \mid 0}(x_t \mid x_0)$ satisfy:</p>

\[\frac{\partial p_{t \mid 0}(x_t \mid x_0)}{\partial t} = - \nabla_x \cdot (p_{t \mid 0} (x_t \mid x_0) v_{t \mid 0}) \tag{6}\]

<p>Here $\nabla_x \cdot = \text{div}(\cdot)$ is the divergence operator.</p>

<p>By Eq. (1), $p_t(x_t)$ satisfy:</p>

<p>\(\begin{align}\frac{\partial p_t(x_t)}{\partial t} &amp;= \frac{\partial}{\partial t} \int p_t(x_t \mid x_0) \pi_0 dx_0\\ &amp;= \int \frac{\partial}{\partial t} p_t(x_t \mid x_0) \pi_0 dx_0 \\&amp;= \int - \nabla_x \cdot (p_{t \mid 0} v_{t \mid 0}) \pi_0 dx_0 \\&amp;= - \nabla_x \cdot \int v_{t \mid 0} p_{t \mid 0} \pi_0 dx_0 \\&amp;= - \nabla_x \cdot \big[p_t\int v_{t \mid 0} \frac{p_{t \mid 0}\pi_0}{p_t} dx_0   \big] \\ &amp;=  - \nabla_x \cdot (p_t \mathbb{E}[v_t \mid x_0]) \end{align}\)
Therefore, the related ODE of $p_t$ is Eq. (4).</p>

<h1 id="training">Training</h1>

<p>The conditional expectation $\mathbb{E}[f(\cdot) \mid x_t]$ is the minimizer of the following objective:</p>

\[\mathbb{E}[f (\cdot) \mid x_t] = \arg \min  \mathbb{E}_{x_0 \sim \pi_0} \mathbb{E}_{x_t \sim p(x_t \mid x_0)} \Vert D(x_t, t) -f(\cdot)\Vert^2 \tag{7}\]

<p>Eq. (7) connect the conditional expectation with a minimizer, which is the objective for training.</p>

<h1 id="conditional-expectation-decomposition">Conditional expectation decomposition</h1>

<p>The conditional expectation $\mathbb{E}[v_{t \mid 0} \mid x_t]$ can be decomposed:</p>

\[\mathbb{E}[v_{t \mid 0} \mid x_t] = \mathbb{E}[\dot{\alpha_t}x_0 + \dot{\sigma}_t \epsilon \mid x_t] = \dot{\alpha_t} \mathbb{E}[x_0 \mid x_t] + \dot{\sigma}_t \mathbb{E}[\epsilon \mid x_t] \tag{8}\]

<p>Since $\mathbb{E}[x_t \mid x_t]=x_t$, therefore, the following equation also holds:</p>

\[\mathbb{E}[x_t \mid x_t] = \mathbb{E}[\alpha_t x_0 + \beta_t \epsilon \mid x_t]=\alpha_t \mathbb{E}[x_0 \mid x_t] + \sigma_t \mathbb{E}[\epsilon \mid x_t] \tag{9}\]

<p>Eq. (8) and (9) are two equations for three terms: $\mathbb{E}[v_{t \mid 0} \mid x_t]$, $\mathbb{E}[x_0 \mid x_t]$ and $\mathbb{E}[\epsilon_t\mid x_t]$. In other words, the other two can be determined as we know one of them. This is the foundation for $x_0$-prediction, $v$-prediction and $\epsilon$-prediction.</p>

<h1 id="connection-with-the-score">Connection with the score</h1>
<p>The score $\nabla_{x_t} \log p_t(x_t)$ is equivalent to:
\(\nabla_{x_t} \log p_t(x_t) = \mathbb{E}[\frac{\alpha_t x_0 -x_t}{\sigma_t^2} \mid x_t] = - \frac{1}{\sigma_t} \mathbb{E}[\epsilon \mid x_t] \tag{10}\)</p>

<p>Proof. 
For a Gaussian transition kernel $p(x_t \mid x_0) = \mathcal{N} (x_t; \alpha_t x_0, \sigma_t^2 \mathbb{I} )$, the conditional score satisfy:</p>

\[\nabla _{x_t} p_t(x_t \vert x_0) = - \frac{x_t - \alpha_t x_0}{\sigma_t^2} p_t(x_t \vert x_0)\]

<p>Thus,</p>

\[\begin{align}\nabla_{x_t} \log p_t(x_t) &amp;= \frac{\nabla_{x_t}p_t(x_t)}{p_t}  \\ &amp;= \frac{\nabla_{x_t} \int p_t(x_t \mid x_0) \pi_0(x_0) dx_0}{p_t} \\ &amp;= \frac{ \int \nabla_{x_t}  p_t(x_t \mid x_0) \pi_0(x_0) dx_0}{p_t} \\ &amp;=  \int - \frac{x_t - \alpha_t x_0}{\sigma_t^2} \frac{p_t(x_t \mid x_0) \pi_0(x_0)}{p_t}  dx_0 \\ &amp;= \mathbb{E} [\frac{\alpha_t x_0 - x_t}{\sigma_t^2} \mid x_t]\end{align}\]

<p>Here we connect the score with conditional expectation. Eq. (8),  (9) and (10) tell us that $\mathbb{E}[v_{t \mid 0} \mid x_t]$, $\mathbb{E}[x_0 \mid x_t]$ and $\mathbb{E}[\epsilon_t\mid x_t]$ and the score $\nabla_{x_t} p_t(x_t)$ can be determined as one of them is known. The score is useful as we want to adding stochasticity at inference time, i.e., using SDE instead of ODE at inference time.</p>

<h1 id="stochasticity-at-inference-time">Stochasticity at inference time</h1>

<p><strong>Lemma 1</strong>. Consider a continuous dynamics given by ODE of the form: $dX_t = u_t dt$, with the density evolution $p_t (X_t)$. Then there exists forward SDEs and backward SDEs that match the marginal distribution $p_t$. The forward SDEs are given by: $dX_t = (u_t +  \frac12 \zeta_t^2 \nabla_{x_t}\log p_t )dt + \zeta_t d W_t$, where $W_t$ is Wiener process. The backward SDEs are given by: $dX_t = (u_t - \frac12 \zeta_t^2 \nabla_{x_t} \log p_t) dt + \zeta_t d W_t$.</p>

<p>Lemma 1 tell us that there exists a series of SDEs that share the same marginal of an ODE. Therefore, we can extend ODE in Eq. (4) to SDEs:
\(dX_t = (v_t(x_t)- \frac12 \zeta_t^2 \nabla_{x_t} \log p_t)dt + \zeta_t dW_t \tag{11}\)
Note that $v_t(x_t)$ and $\nabla_{x_t} \log p_t(x_t)$ can be determined as one of them is known.</p>]]></content><author><name></name></author><category term="reading notes" /><summary type="html"><![CDATA[The conditional expectation is the core to connect almost all things in diffusion models, e.g., the sampling ODE, SDE, training objective, score function and so on. This note try to understand diffusion models by conditional expecation.]]></summary></entry><entry><title type="html">Maximum Likelihood = Minimize KL Divergence</title><link href="http://localhost:4000/blogs/2025/11/10/ML-KL.html" rel="alternate" type="text/html" title="Maximum Likelihood = Minimize KL Divergence" /><published>2025-11-10T00:00:00-08:00</published><updated>2025-11-10T00:00:00-08:00</updated><id>http://localhost:4000/blogs/2025/11/10/ML-KL</id><content type="html" xml:base="http://localhost:4000/blogs/2025/11/10/ML-KL.html"><![CDATA[<p>When we estimate parameters by <strong>Maximum Likelihood Estimation (MLE)</strong>,<br />
we’re actually minimizing the <strong>Kullback–Leibler divergence</strong> between the true data distribution and our model’s distribution.</p>

<hr />

<h2 id="setup">Setup</h2>

<p>Let the true data distribution be $p_\text{data}(x)$ (unknown),<br />
and our model distribution be $p_\theta(x)$ with parameters $\theta$.</p>

<p>MLE seeks:
\(\theta^* = \arg\max_\theta \; \mathbb{E}_{x \sim p_\text{data}} [ \log p_\theta(x) ].\)</p>

<hr />

<h2 id="link-to-kl-divergence">Link to KL Divergence</h2>

<p>The <strong>KL divergence</strong> from $p_\text{data}$ to $p_\theta$ is:</p>

\[D_{\mathrm{KL}}(p_\text{data} \Vert p_\theta)
= \mathbb{E}_{x \sim p_\text{data}} \left[ \log \frac{p_\text{data}(x)}{p_\theta(x)} \right].\]

<p>We can expand this as:</p>

\[D_{\mathrm{KL}}(p_\text{data} \Vert p_\theta)
= \mathbb{E}_{p_\text{data}}[\log p_\text{data}(x)]
- \mathbb{E}_{p_\text{data}}[\log p_\theta(x)].\]

<p>The first term doesn’t depend on $\theta$,<br />
so minimizing $D_{\mathrm{KL}}$ is equivalent to <strong>maximizing</strong> the expected log-likelihood!</p>

<hr />

<h2 id="interpretation">Interpretation</h2>

<ul>
  <li><strong>MLE</strong>: Find the model that makes the observed data most probable.</li>
  <li><strong>KL minimization</strong>: Find the model closest (in information distance) to the true distribution.</li>
</ul>

<p>They are the <strong>same optimization</strong>, viewed from two lenses:</p>
<blockquote>
  <p>MLE ≡ minimize $D_{\mathrm{KL}}(p_\text{data} \Vert p_\theta)$</p>
</blockquote>

<hr />]]></content><author><name></name></author><category term="reading notes" /><summary type="html"><![CDATA[When we estimate parameters by Maximum Likelihood Estimation (MLE), we’re actually minimizing the Kullback–Leibler divergence between the true data distribution and our model’s distribution.]]></summary></entry></feed>